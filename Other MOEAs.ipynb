{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6996159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygmo as pg\n",
    "import random\n",
    "import functools\n",
    "import math as m\n",
    "import copy\n",
    "from numpy import genfromtxt\n",
    "\n",
    "# Geometry modules\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as patches \n",
    "import seaborn as sns\n",
    "\n",
    "# Misc\n",
    "import time \n",
    "from time import process_time\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial import ConvexHull\n",
    "import secrets\n",
    "\n",
    "# skopt\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt import BayesSearchCV, space, plots\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt import gp_minimize\n",
    "\n",
    "# sklearn modules\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay, precision_recall_curve, recall_score\n",
    "from sklearn.metrics import make_scorer, precision_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "from deap import algorithms, base, creator, tools\n",
    "\n",
    "\n",
    "# For ignoring warnings about bad classifiers - it's the nature of the algorithm to come across these\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Matplotlib configuration\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8,6)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "plt.rc('axes', titlesize=12)\n",
    "plt.rc('axes', labelsize=12)\n",
    "\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17f432",
   "metadata": {},
   "source": [
    "## Declare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec009f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 'churn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29eb856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.read_csv(f'UCI/{data_set}/X_train.csv')\n",
    "\n",
    "X_train = pd.read_csv(f'UCI/{data_set}/X_train.csv').values\n",
    "X_test = pd.read_csv(f'UCI/{data_set}/X_test.csv').values\n",
    "y_train = genfromtxt(f'UCI/{data_set}/y_train.csv', delimiter=',').astype(int)\n",
    "y_test = genfromtxt(f'UCI/{data_set}/y_test.csv', delimiter=',').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c6553d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare standard scaler\n",
    "scaler = StandardScaler()\n",
    "# Transform training and testing data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe3d0813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features in model\n",
    "total_features = int(X_train.shape[1])\n",
    "\n",
    "# Feature names\n",
    "feat_names = np.asarray(list(X_df.columns))\n",
    "\n",
    "# Number of iterations (for all MOEAs in this workbook)\n",
    "num_iters = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d601b",
   "metadata": {},
   "source": [
    "## Declaring classifier to run all MOEAs on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35f48acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of learning algorithms\n",
    "model_dict = {\n",
    "    \"SVC lin\": SVC(random_state=42,kernel='linear',max_iter=int(1e4), probability=True),\n",
    "    \"Log lasso\": LogisticRegression(solver='liblinear',random_state=42,penalty='l1'),\n",
    "    \"Log ridge\": LogisticRegression(random_state=42,penalty='l2'),\n",
    "    \"Gauss NB\": GaussianNB(), # Naive Bayes\n",
    "    \"Dec trees\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Grad boost\": GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42),\n",
    "    \"SVC rbf\": SVC(random_state=42,kernel='rbf', max_iter=int(1e4)),\n",
    "    \"Random forest\": RandomForestClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef349b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_string = 'Log lasso'\n",
    "clf_ = model_dict[model_string]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01aa5c",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96449fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the classification error measure - 'accuracy' or 'recall'\n",
    "metric = 'recall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b5965b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = 'num_features'\n",
    "# objective = 'time'\n",
    "\n",
    "if objective == 'time':\n",
    "    comp_time_all_X = get_model_time(model_string)\n",
    "    print(comp_time_all_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccf77e",
   "metadata": {},
   "source": [
    "## Declare time or number of features as objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "310cdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_time(model_string):\n",
    "    \n",
    "    '''\n",
    "    Returns the seconds of how long a classifier takes to fit the whole data matrix\n",
    "    '''\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        (\"model\", model_dict[model_string])\n",
    "    ])\n",
    "    \n",
    "    times = []\n",
    "    \n",
    "    # Times are inconsistent - get max by looping through several times\n",
    "    for _ in range(100):\n",
    "    \n",
    "        start = process_time()\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        end = process_time()\n",
    "        \n",
    "        times.append(end-start)\n",
    "    \n",
    "    return np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9ecb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare path to save to for all outputs\n",
    "if objective == 'time':\n",
    "    path = 'Other_MOEAs_time'\n",
    "else:\n",
    "    path = 'Other_MOEAs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afeddbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(value):\n",
    "    '''\n",
    "    I: an evaluation value\n",
    "    O: its normalised value \n",
    "    '''\n",
    "\n",
    "    # The worst possible scenario is using all features (max comp time) \n",
    "    nadir = comp_time_all_X\n",
    "    # The best possible scenario is a negligible time\n",
    "    ideal = 1e-8\n",
    "                    \n",
    "    return ((value - ideal) / (nadir - ideal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc5e15",
   "metadata": {},
   "source": [
    "def normalise(value):\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad017b4",
   "metadata": {},
   "source": [
    "## Getting Non-Dom Fronts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5567dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nondom_fronts(data):\n",
    "    \n",
    "    ndf, dl, dc, ndr = pg.fast_non_dominated_sorting(data[:,[1,2]])\n",
    "    \n",
    "    best = data[ndf[0]]\n",
    "    \n",
    "    # Sort by ascending\n",
    "    best_sorted = best[best[:, 0].argsort()[::-1]]\n",
    "    \n",
    "    uniques = np.unique([tuple(row) for row in best_sorted], axis=0)\n",
    "    \n",
    "    uniques = uniques[uniques[:, 0].argsort()[::-1]]\n",
    "    \n",
    "    return uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3fb7f8",
   "metadata": {},
   "source": [
    "# NSGA-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c92c7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsga_all_evals = np.zeros((1, 3 + total_features))\n",
    "\n",
    "def evaluate_individual(individual):\n",
    "    \n",
    "    selected_features = [bool(bit) for bit in individual]\n",
    "\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    X_test_selected = X_test[:, selected_features]    \n",
    "    n_feats = len(np.where(np.asarray(individual) == 1)[0].tolist())\n",
    "    \n",
    "    # Catch error of passing an array of zeros to classifier\n",
    "    if n_feats == 0:\n",
    "        return 0,0\n",
    "        \n",
    "    start = process_time()\n",
    "    model = clf_.fit(X_train_selected, y_train)\n",
    "    end = process_time()\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    \n",
    "    if metric == 'accuracy':\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "    else:\n",
    "        score = recall_score(y_test, y_pred)\n",
    "        \n",
    "    if objective == 'time':\n",
    "        obj2 = normalise(end-start)\n",
    "    else:\n",
    "        obj2 = n_feats\n",
    "\n",
    "    this_eval = np.zeros((1, 3 + total_features))\n",
    "    this_eval[:,:3] = (score, obj2, (1-score))\n",
    "    this_eval[:,3:] = copy.deepcopy(individual)\n",
    "    global nsga_all_evals \n",
    "    nsga_all_evals = np.vstack((nsga_all_evals,this_eval))\n",
    "\n",
    "    return score, obj2\n",
    "\n",
    "# Create the NSGA-II algorithm components\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0, -1.0))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", np.random.randint, 2)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=X_train.shape[1])\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate_individual)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selNSGA2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b921935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.79166667 13.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Set the algorithm parameters\n",
    "population_size = 50\n",
    "max_generations = num_iters\n",
    "offspring_size = population_size\n",
    "\n",
    "# Create the initial population\n",
    "population = toolbox.population(n=population_size)\n",
    "\n",
    "# Run the NSGA-II algorithm\n",
    "for generation in range(max_generations):\n",
    "    offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.1)\n",
    "    fits = toolbox.map(toolbox.evaluate, offspring)\n",
    "    for fit, ind in zip(fits, offspring):\n",
    "        ind.fitness.values = fit\n",
    "    population = toolbox.select(offspring, k=population_size)\n",
    "\n",
    "nsga_nondom_sols = get_nondom_fronts(nsga_all_evals[1:])\n",
    "    \n",
    "print(nsga_nondom_sols[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce1d1475",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f\"{path}/{data_set}_NSGA2.csv\", nsga_nondom_sols, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbec144",
   "metadata": {},
   "source": [
    "# Multi Objective Particle Swarm Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ca10ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mopso_all_evals = np.zeros((1, 3 + total_features))\n",
    "\n",
    "class MOPSOFeatureSelection:\n",
    "    def __init__(self, n_particles, n_iterations, w=0.5, c1=1.5, c2=1.5, n_features=total_features):\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iterations = n_iterations\n",
    "        self.w = w # inertia\n",
    "        self.c1 = c1 # cognitive\n",
    "        self.c2 = c2 # social\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def run(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.n_features = X_train.shape[1] if self.n_features is None else self.n_features\n",
    "\n",
    "        self.particles = np.random.randint(0, 2, size=(self.n_particles, self.n_features))\n",
    "        self.velocities = np.random.uniform(size=(self.n_particles, self.n_features))\n",
    "\n",
    "        self.pbest = np.copy(self.particles)\n",
    "        self.gbest = self.get_global_best()\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            self.update_particles()\n",
    "            self.update_pbest()\n",
    "            self.update_gbest()\n",
    "    \n",
    "    # Sets random individual with highest accuracy as the first globals\n",
    "    def get_global_best(self):\n",
    "        scores = [self.evaluate(chromosome)[1] for chromosome in self.pbest]\n",
    "        gbest_idx = np.argmax(scores)\n",
    "        return self.pbest[gbest_idx]\n",
    "\n",
    "    def evaluate(self, chromosome):\n",
    "        if not any(chromosome):\n",
    "            # To avoid evaluating an individual with all zeros\n",
    "            return 0.0, 0.0\n",
    "\n",
    "        selected_features = np.where(chromosome == 1)[0]\n",
    "        if len(selected_features) == 0:\n",
    "            # At least one feature should be selected\n",
    "            return 0.0, 0.0\n",
    "        global clf_\n",
    "        start = process_time()\n",
    "        clf_.fit(X_train[:, selected_features], y_train)\n",
    "        end = process_time()\n",
    "        y_pred = clf_.predict(X_test[:, selected_features])\n",
    "        \n",
    "        if metric == 'accuracy':\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "        else:\n",
    "            accuracy = recall_score(y_test, y_pred)\n",
    "            \n",
    "        if objective == 'time':\n",
    "            obj2 = normalise(end-start)\n",
    "        else:\n",
    "            obj2 = len(selected_features)\n",
    "        \n",
    "        # For keeping track of all evaluations and doing non-dom sort\n",
    "        this_eval = np.zeros((1, 3 + self.n_features))\n",
    "        this_eval[:,:3] = (accuracy, obj2, (1-accuracy))\n",
    "        this_eval[:,3:] = copy.deepcopy(chromosome)\n",
    "        global mopso_all_evals \n",
    "        mopso_all_evals = np.vstack((mopso_all_evals,this_eval))\n",
    "        \n",
    "        return accuracy, obj2\n",
    "\n",
    "    def update_particles(self):\n",
    "        r1, r2 = np.random.uniform(size=(2, self.n_particles, self.n_features))\n",
    "        cognitive_component = self.c1 * r1 * (self.pbest - self.particles)\n",
    "        social_component = self.c2 * r2 * (self.gbest - self.particles)\n",
    "        self.velocities = self.w * self.velocities + cognitive_component + social_component\n",
    "        self.particles = (self.velocities > 0.5).astype(int)\n",
    "\n",
    "    def update_pbest(self):\n",
    "        pbest_values = [self.evaluate(chromosome) for chromosome in self.pbest]\n",
    "        current_values = [self.evaluate(chromosome) for chromosome in self.particles]\n",
    "\n",
    "        for i, (pbest_value, current_value) in enumerate(zip(pbest_values, current_values)):\n",
    "            if current_value[0] > pbest_value[0] and current_value[1] < pbest_value[1]:\n",
    "                self.pbest[i] = np.copy(self.particles[i])\n",
    "\n",
    "    def update_gbest(self):\n",
    "        gbest_value = self.evaluate(self.gbest)\n",
    "        for i in range(self.n_particles):\n",
    "            current_value = self.evaluate(self.particles[i])\n",
    "            if current_value[0] > gbest_value[0] and current_value[1] < gbest_value[1]:\n",
    "                self.gbest = np.copy(self.particles[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8dd3f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79166667 9.        ]\n",
      " [0.79166667 9.        ]\n",
      " [0.         8.        ]\n",
      " [0.         8.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Driver code\n",
    "if __name__ == \"__main__\":\n",
    "    n_particles = 20\n",
    "    n_iterations = num_iters\n",
    "\n",
    "    MOPSO = MOPSOFeatureSelection(n_particles, n_iterations)\n",
    "    \n",
    "    MOPSO.run(X_train, y_train)\n",
    "\n",
    "    mopso_nondom_sols = get_nondom_fronts(mopso_all_evals[1:])\n",
    "    \n",
    "    print(mopso_nondom_sols[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5d8d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f\"{path}/{data_set}_MOPSO.csv\", mopso_nondom_sols, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717cd384",
   "metadata": {},
   "source": [
    "# Multi Objective Differential Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf45ab",
   "metadata": {},
   "source": [
    "The main difference between a generic genetic algorithm and a differential evolution algorithm is the way that new solutions are created. In a genetic algorithm, new solutions are created by mutating existing solutions. This can be done by randomly changing the values of some of the genes in the solution, or by swapping genes between two different solutions.\n",
    "\n",
    "In a differential evolution algorithm, new solutions are created by using a vectorized mutation operator. This operator takes three existing solutions and adds a weighted difference between two of them to the third one. This creates a new solution that is hopefully better than the existing solutions.\n",
    "\n",
    "The DE_mutate() operator first checks if the mutation rate, cr, is greater than a random number. If it is, then the mutation operator does not mutate the individual. Otherwise, the mutation operator randomly selects three individuals from the population. The difference between two of the individuals is then calculated. The new solution is then created by replacing the genes in the individual with the genes from the difference vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4645f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_all_evals = np.zeros((1, 3 + total_features))\n",
    "\n",
    "def evaluate_individual(individual):\n",
    "    \n",
    "    \n",
    "    selected_features = [bool(bit) for bit in individual]\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    X_test_selected = X_test[:, selected_features]    \n",
    "    n_feats = len(np.where(np.asarray(individual) == 1)[0].tolist())\n",
    "    \n",
    "    if n_feats == 0:\n",
    "        return 0,0\n",
    "        \n",
    "    start = process_time()\n",
    "    model = clf_.fit(X_train_selected, y_train)\n",
    "    end = process_time()\n",
    "    \n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    \n",
    "    if metric == 'accuracy':\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "    else:\n",
    "        score = recall_score(y_test, y_pred)\n",
    "        \n",
    "    if objective == 'time':\n",
    "        obj2 = normalise(end-start)\n",
    "    else:\n",
    "        obj2 = n_feats\n",
    "    \n",
    "    this_eval = np.zeros((1, 3 + total_features))\n",
    "    this_eval[:,:3] = (score, obj2, (1-score))\n",
    "    this_eval[:,3:] = copy.deepcopy(individual)\n",
    "    global de_all_evals \n",
    "    de_all_evals = np.vstack((de_all_evals,this_eval))\n",
    "\n",
    "    return score, obj2\n",
    "\n",
    "def DE_mutate(individual, population, cr=0.5, F=0.7):\n",
    "    if random.random() < cr:\n",
    "        return individual\n",
    "\n",
    "    a, b, c = random.sample(population, 3)\n",
    "    diff = np.asarray(b) - np.asarray(a)\n",
    "    mutant = []\n",
    "    for i in range(len(individual)):\n",
    "        if random.random() < F:\n",
    "            mutant.append(1 if individual[i] == 0 else 0)\n",
    "        else:\n",
    "            mutant.append(individual[i])\n",
    "            \n",
    "        return mutant\n",
    "\n",
    "# Create the DEAP toolbox\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0, -1.0))  # Maximizing score, minimizing feat number\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", np.random.randint, 2)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=total_features)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Register the evaluation and mate functions\n",
    "toolbox.register(\"evaluate\", evaluate_individual)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", DE_mutate)  \n",
    "toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "\n",
    "def main(population_size, generations):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Initialize population\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    pareto_fronts = []\n",
    "\n",
    "    for gen in range(generations):\n",
    "        # Evaluate individuals\n",
    "        fitnesses = [toolbox.evaluate(ind) for ind in population]\n",
    "        for ind, fit in zip(population, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population))\n",
    "\n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        for mutant in offspring:\n",
    "            toolbox.mutate(mutant, offspring)\n",
    "            del mutant.fitness.values\n",
    "            \n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if np.random.rand() < 0.5:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        # Evaluate the offspring\n",
    "        fitnesses = [toolbox.evaluate(ind) for ind in offspring]\n",
    "        for ind, fit in zip(offspring, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa0e39cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.79166667 12.        ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    MODE = main(population_size=100, generations=num_iters)\n",
    "    de_nondom_sols = get_nondom_fronts(de_all_evals[1:])\n",
    "    print(de_nondom_sols[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ca49b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f\"{path}/{data_set}_MODE.csv\", de_nondom_sols, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28edbced",
   "metadata": {},
   "source": [
    "## Strength Pareto Evolutionary Algorithm II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9daf0a8",
   "metadata": {},
   "source": [
    "SPEA2 and NSGA2 are both multi-objective optimization algorithms that are based on the concept of Pareto dominance. However, there are some key differences between the two algorithms.\n",
    "\n",
    "SPEA2 uses fitness sharing, while NSGA2 uses crowding distance. Fitness sharing is a technique that penalizes individuals that are too similar to each other. This helps to maintain a diverse population of individuals, which is important for multi-objective optimization. Crowding distance is a measure of how crowded an individual is in the objective space. Individuals with a high crowding distance are more likely to be selected for the next generation, as they are more likely to represent a unique point in the objective space.\n",
    "\n",
    "SPEA2 uses a weighted sum of the objectives, while NSGA2 uses a Pareto ranking. SPEA2 calculates the fitness of an individual as a weighted sum of the objectives. This means that the algorithm can be biased towards one or more of the objectives. NSGA2, on the other hand, uses a Pareto ranking. This means that the algorithm does not favor any of the objectives, and it tries to find a set of individuals that are all Pareto optimal.\n",
    "\n",
    "In general, SPEA2 is considered to be more robust than NSGA2. This is because SPEA2 is less likely to get stuck in local optima. However, NSGA2 is generally faster than SPEA2.\n",
    "\n",
    "This algorithm uses deap's eaMuPlusLambda function, which first creates a new population of solutions by selecting mu parents from the current population and performing crossover on them. The new population is then mutated with probability mutpb. The function then repeats this process until a stopping criterion is met. The stopping criterion can be a maximum number of generations, a maximum time limit, or a certain fitness threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c9ee146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    \"\"\"\n",
    "    Evaluates an individual by first selecting the features that are turned on in the individual,\n",
    "    then applying feature selection to the training and test data, and finally training a classifier and calculating the accuracy.\n",
    "    The function returns the number of selected features and the negative accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    selected_features = [i for i, val in enumerate(individual) if val]\n",
    "    if not selected_features:\n",
    "        return 0, 0,  # Return 0 accuracy and 0 features if none are selected\n",
    "    \n",
    "    n_feats = len(np.where(np.asarray(individual) == 1)[0].tolist())\n",
    "\n",
    "    # Apply feature selection\n",
    "    selected_X_train = X_train[:, selected_features]\n",
    "    selected_X_test = X_test[:, selected_features]\n",
    "\n",
    "    # Train a classifier\n",
    "    start = process_time()\n",
    "    model = clf_.fit(selected_X_train, y_train)\n",
    "    end = process_time()\n",
    "\n",
    "    y_pred = model.predict(selected_X_test)\n",
    "    \n",
    "    if metric == 'accuracy':\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    else:\n",
    "        accuracy = recall_score(y_test, y_pred)\n",
    "        \n",
    "    if objective == 'time':\n",
    "        obj2 = normalise(end-start)\n",
    "    else:\n",
    "        obj2 = n_feats\n",
    "        \n",
    "    # Return negative accuracy to maximize it\n",
    "    return -accuracy, obj2\n",
    "\n",
    "# DEAP initialization\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0, -1.0))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=total_features)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Genetic operators\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selSPEA2) # this selection operator makes this an SPEA2\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "\n",
    "# Set up and run the algorithm\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Sets up and runs the SPEA-2 algorithm.\n",
    "    It first creates a random population of individuals,\n",
    "    then uses the `eaMuPlusLambda()` algorithm to evolve the population over 50 generations.\n",
    "    The algorithm uses the `cxTwoPoint()` and `mutFlipBit()` genetic operators to create new individuals,\n",
    "    and the `selSPEA2()` selection operator to select individuals for the next generation.\n",
    "\n",
    "    The main function then displays the Pareto front solutions, which are the individuals that have the best trade-off between the number of selected features and the accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(42)\n",
    "    \n",
    "    pop = toolbox.population(n=50)\n",
    "    hof = tools.ParetoFront()\n",
    "\n",
    "    algorithms.eaMuPlusLambda(pop, toolbox, mu=50, lambda_=100, cxpb=0.7, mutpb=0.3, ngen=num_iters, stats=None,\n",
    "                              halloffame=hof, verbose=False)\n",
    "\n",
    "    return hof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7924acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79166667 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Hall of fame is a functionality in DEAP that returns the Pareto fittest individuals\n",
    "    hof = main()\n",
    "    \n",
    "    spea2_nondom_sols = np.zeros((len(hof),3+total_features))\n",
    "    \n",
    "    # Write the Pareto front solutions into a matrix\n",
    "    for i, ind in enumerate(hof):\n",
    "        \n",
    "        score = -ind.fitness.values[0]\n",
    "        \n",
    "        obj2 = ind.fitness.values[1]\n",
    "        \n",
    "        # Write in the same universal results matrix format\n",
    "        spea2_nondom_sols[i,:3] = (score, obj2, 1-score)\n",
    "        \n",
    "        spea2_nondom_sols[i,3:] = ind\n",
    "        \n",
    "# Algorithm sometimes returns zeros as results, this gets rid of them\n",
    "for i, point in enumerate(spea2_nondom_sols):\n",
    "\n",
    "    if point[0] <=0:\n",
    "        spea2_nondom_sols = np.delete(spea2_nondom_sols, i, 0)\n",
    "        \n",
    "print(spea2_nondom_sols[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eefaaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f\"{path}/{data_set}_SPEA2.csv\", spea2_nondom_sols, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095c325",
   "metadata": {},
   "source": [
    "# Parameter Search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
